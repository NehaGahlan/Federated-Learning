# -*- coding: utf-8 -*-
"""GSR_features_AMIGOS-40sec_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pnPb_Yf4Z0AE71ciCK9xEJ3jy-RewipH
"""

from google.colab import drive
drive.mount('/content/drive')

ls /content/drive/MyDrive/Datasets/Amigos/Mat_toCSV/User29/short_videos

import pandas as pd

"""# **EDA 01**"""

!pip install heartpy
!pip install pyhrv
!pip install biosppy
!pip install pyhrv
!pip install hrv-analysis

import os
import sys
import csv


import numpy as np
import sympy as sp
import weakref
import matplotlib.pyplot as plt
import math
import statistics


#median
import statistics
from statistics import median

import os
from glob import glob

# data science
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
import seaborn as sns

# signal processing
from scipy import signal
from scipy.ndimage import label
from scipy.stats import zscore
from scipy.interpolate import interp1d
from scipy.integrate import trapz
from scipy.signal import butter, filtfilt, tf2zpk


#import heartpy as hp
import numpy as np
import pandas as pd
pd.set_option('display.max_colwidth',100000)
import cvxopt as cv
import cvxopt.solvers

import scipy.stats as sp

#from hrvanalysis import remove_outliers, remove_ectopic_beats, interpolate_nan_values
#from hrvanalysis import *

def detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising',
                 kpsh=False, valley=False, show=False, ax=None):

    """Detect peaks in data based on their amplitude and other features.
    Parameters
    ----------
    x : 1D array_like
        data.
    mph : {None, number}, optional (default = None)
        detect peaks that are greater than minimum peak height.
    mpd : positive integer, optional (default = 1)
        detect peaks that are at least separated by minimum peak distance (in
        number of data).
    threshold : positive number, optional (default = 0)
        detect peaks (valleys) that are greater (smaller) than `threshold`
        in relation to their immediate neighbors.
    edge : {None, 'rising', 'falling', 'both'}, optional (default = 'rising')
        for a flat peak, keep only the rising edge ('rising'), only the
        falling edge ('falling'), both edges ('both'), or don't detect a
        flat peak (None).
    kpsh : bool, optional (default = False)
        keep peaks with same height even if they are closer than `mpd`.
    valley : bool, optional (default = False)
        if True (1), detect valleys (local minima) instead of peaks.
    show : bool, optional (default = False)
        if True (1), plot data in matplotlib figure.
    ax : a matplotlib.axes.Axes instance, optional (default = None).
    Returns
    -------
    ind : 1D array_like
        indeces of the peaks in `x`.
    Notes
    -----
    The detection of valleys instead of peaks is performed internally by simply
    negating the data: `ind_valleys = detect_peaks(-x)`
    
    The function can handle NaN's 
    See this IPython Notebook [1]_.
    References
    ----------
    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb
    Examples
    --------
    >>> from detect_peaks import detect_peaks
    >>> x = np.random.randn(100)
    >>> x[60:81] = np.nan
    >>> # detect all peaks and plot data
    >>> ind = detect_peaks(x, show=True)
    >>> print(ind)
    >>> x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5
    >>> # set minimum peak height = 0 and minimum peak distance = 20
    >>> detect_peaks(x, mph=0, mpd=20, show=True)
    >>> x = [0, 1, 0, 2, 0, 3, 0, 2, 0, 1, 0]
    >>> # set minimum peak distance = 2
    >>> detect_peaks(x, mpd=2, show=True)
    >>> x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5
    >>> # detection of valleys instead of peaks
    >>> detect_peaks(x, mph=0, mpd=20, valley=True, show=True)
    >>> x = [0, 1, 1, 0, 1, 1, 0]
    >>> # detect both edges
    >>> detect_peaks(x, edge='both', show=True)
    >>> x = [-2, 1, -2, 2, 1, 1, 3, 0]
    >>> # set threshold = 2
    >>> detect_peaks(x, threshold = 2, show=True)
    """

    x = np.atleast_1d(x).astype('float64')
    if x.size < 3:
        return np.array([], dtype=int)
    if valley:
        x = -x
    # find indices of all peaks
    dx = x[1:] - x[:-1]
    # handle NaN's
    indnan = np.where(np.isnan(x))[0]
    if indnan.size:
        x[indnan] = np.inf
        dx[np.where(np.isnan(dx))[0]] = np.inf
    ine, ire, ife = np.array([[], [], []], dtype=int)
    if not edge:
        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]
    else:
        if edge.lower() in ['rising', 'both']:
            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]
        if edge.lower() in ['falling', 'both']:
            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]
    ind = np.unique(np.hstack((ine, ire, ife)))
    # handle NaN's
    if ind.size and indnan.size:
        # NaN's and values close to NaN's cannot be peaks
        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]
    # first and last values of x cannot be peaks
    if ind.size and ind[0] == 0:
        ind = ind[1:]
    if ind.size and ind[-1] == x.size-1:
        ind = ind[:-1]
    # remove peaks < minimum peak height
    if ind.size and mph is not None:
        ind = ind[x[ind] >= mph]
    # remove peaks - neighbors < threshold
    if ind.size and threshold > 0:
        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)
        ind = np.delete(ind, np.where(dx < threshold)[0])
    # detect small peaks closer than minimum peak distance
    if ind.size and mpd > 1:
        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height
        idel = np.zeros(ind.size, dtype=bool)
        for i in range(ind.size):
            if not idel[i]:
                # keep peaks with the same height if kpsh is True
                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \
                    & (x[ind[i]] > x[ind] if kpsh else True)
                idel[i] = 0  # Keep current peak
        # remove the small peaks and sort back the indices by their occurrence
        ind = np.sort(ind[~idel])

    if show:
        if indnan.size:
            x[indnan] = np.nan
        if valley:
            x = -x
        _plot(x, mph, mpd, threshold, edge, valley, ax, ind)

    return ind

def movingAvg(dt):
    N = 16
    cumsum, moving_aves = [0], []
    for i, x in enumerate(dt, 1):
        cumsum.append(cumsum[i-1] + x)
        if i>=N:
            moving_ave = (cumsum[i] - cumsum[i-N])/N
            moving_aves.append(moving_ave)
    return moving_aves

def cvxEDA(y, delta, tau0=2., tau1=0.7, delta_knot=10., alpha=8e-4, gamma=1e-2,
           solver=None, options={'reltol':1e-9}):
    """CVXEDA Convex optimization approach to electrodermal activity processing
    This function implements the cvxEDA algorithm described in "cvxEDA: a
    Convex Optimization Approach to Electrodermal Activity Processing"
    (http://dx.doi.org/10.1109/TBME.2015.2474131, also available from the
    authors' homepages).
    Arguments:
       y: observed EDA signal (we recommend normalizing it: y = zscore(y))
       delta: sampling interval (in seconds) of y
       tau0: slow time constant of the Bateman function
       tau1: fast time constant of the Bateman function
       delta_knot: time between knots of the tonic spline function
       alpha: penalization for the sparse SMNA driver
       gamma: penalization for the tonic spline coefficients
       solver: sparse QP solver to be used, see cvxopt.solvers.qp
       options: solver options, see:
                http://cvxopt.org/userguide/coneprog.html#algorithm-parameters
    Returns (see paper for details):
       r: phasic component
       p: sparse SMNA driver of phasic component
       t: tonic component
       l: coefficients of tonic spline
       d: offset and slope of the linear drift term
       e: model residuals
       obj: value of objective function being minimized (eq 15 of paper)
    """

    n = len(y)
    y = cv.matrix(y)

    # bateman ARMA model
    a1 = 1./min(tau1, tau0) # a1 > a0
    a0 = 1./max(tau1, tau0)
    ar = np.array([(a1*delta + 2.) * (a0*delta + 2.), 2.*a1*a0*delta**2 - 8.,
        (a1*delta - 2.) * (a0*delta - 2.)]) / ((a1 - a0) * delta**2)
    ma = np.array([1., 2., 1.])

    # matrices for ARMA model
    i = np.arange(2, n)
    A = cv.spmatrix(np.tile(ar, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))
    M = cv.spmatrix(np.tile(ma, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))

    # spline
    delta_knot_s = int(round(delta_knot / delta))
    spl = np.r_[np.arange(1.,delta_knot_s), np.arange(delta_knot_s, 0., -1.)] # order 1
    spl = np.convolve(spl, spl, 'full')
    spl /= max(spl)
    # matrix of spline regressors
    i = np.c_[np.arange(-(len(spl)//2), (len(spl)+1)//2)] + np.r_[np.arange(0, n, delta_knot_s)]
    nB = i.shape[1]
    j = np.tile(np.arange(nB), (len(spl),1))
    p = np.tile(spl, (nB,1)).T
    valid = (i >= 0) & (i < n)
    B = cv.spmatrix(p[valid], i[valid], j[valid])

    # trend
    C = cv.matrix(np.c_[np.ones(n), np.arange(1., n+1.)/n])
    nC = C.size[1]

    # Solve the problem:
    # .5*(M*q + B*l + C*d - y)^2 + alpha*sum(A,1)*p + .5*gamma*l'*l
    # s.t. A*q >= 0

    old_options = cv.solvers.options.copy()
    cv.solvers.options.clear()
    cv.solvers.options.update(options)
    if solver == 'conelp':
        # Use conelp
        z = lambda m,n: cv.spmatrix([],[],[],(m,n))
        G = cv.sparse([[-A,z(2,n),M,z(nB+2,n)],[z(n+2,nC),C,z(nB+2,nC)],
                    [z(n,1),-1,1,z(n+nB+2,1)],[z(2*n+2,1),-1,1,z(nB,1)],
                    [z(n+2,nB),B,z(2,nB),cv.spmatrix(1.0, range(nB), range(nB))]])
        h = cv.matrix([z(n,1),.5,.5,y,.5,.5,z(nB,1)])
        c = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T,z(nC,1),1,gamma,z(nB,1)])
        res = cv.solvers.conelp(c, G, h, dims={'l':n,'q':[n+2,nB+2],'s':[]})
        obj = res['primal objective']
    else:
        # Use qp
        Mt, Ct, Bt = M.T, C.T, B.T
        H = cv.sparse([[Mt*M, Ct*M, Bt*M], [Mt*C, Ct*C, Bt*C], 
                    [Mt*B, Ct*B, Bt*B+gamma*cv.spmatrix(1.0, range(nB), range(nB))]])
        f = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T - Mt*y,  -(Ct*y), -(Bt*y)])
        res = cv.solvers.qp(H, f, cv.spmatrix(-A.V, A.I, A.J, (n,len(f))),
                            cv.matrix(0., (n,1)), solver=solver)
        obj = res['primal objective'] + .5 * (y.T * y)
    cv.solvers.options.clear()
    cv.solvers.options.update(old_options)

    l = res['x'][-nB:]
    d = res['x'][n:n+nC]
    t = B*l + C*d
    q = res['x'][:n]
    p = A * q
    r = M * q
    e = y - r - t

    return (np.array(a).ravel() for a in (r, p, t, l, d, e, obj))

def process(y,delta):
    [r, p, t, l, d, e, obj] = cvxEDA(y, delta)
    smoothed_phasic=movingAvg(r)
    phasic_peaks=detect_peaks(smoothed_phasic, mph=0.001, mpd=int(1/delta), show=False)
    temp={}
    #Calculate the moments of the GSR data
    temp["mean_gsr"]=np.mean(y)
    temp["var_gsr"]=np.var(y)
    temp["skew_gsr"]=sp.skew(y)
    temp["kurtosis_gsr"]=sp.kurtosis(y)
    temp["std_gsr"]=np.std(y)
    #Calculate the moments of the SCL data (tonic)
    temp["mean_scl"]=np.mean(t)
    temp["var_scl"]=np.var(t)
    temp["skew_scl"]=sp.skew(t)
    temp["kurtosis_scl"]=sp.kurtosis(t)
    temp["std_scl"]=np.std(t)
    temp["slope_scl"]=np.max(t)-np.min(t)
    #Calculate the moments of the SCR data (phasic)
    temp["mean_scr"]=np.mean(r)
    temp["var_scr"]=np.var(r)
    temp["skew_scr"]=sp.skew(r)
    temp["kurtosis_scr"]=sp.kurtosis(r)
    temp["std_scr"]=np.std(r)
    temp["max_scr"]=np.max(r)
    temp["scr_peaks"]=len(phasic_peaks)
    return temp

pip install pyedflib

df=pd.read_csv('/content/drive/MyDrive/Datasets/Amigos/Mat_toCSV/User32/short_videos/user32_vid15.csv')
df2 = df.tail(6400)
df2.shape

def windowing(df,windowsizerow):
  column_names=['mean_gsr','var_gsr','skew_gsr','kurtosis_gsr','std_gsr','mean_scl','var_scl','skew_scl','kurtosis_scl','std_scl','slope_scl','mean_scr','var_scr','skew_scr','kurtosis_scr','std_scr','max_scr']
  result_df= pd.DataFrame(columns=column_names) 
  df1=df.rolling(window=windowsizerow)
  type(df1)
  i=1
  j=0
  for df2 in df1:
    if len(df2)>=windowsizerow and i%int(windowsizerow/1)==0:
     results=process(df2.GSR,1./128)
     result_df=result_df.append(results,ignore_index=True)
      # print(result_df)
    i+=1
    j+=1
    
  return result_df

from pyedflib import highlevel
file = 'user32_vid15.csv'
#sampling_freq = 256
#sampling_duration= 329
#number_of_samples = sampling_freq * sampling_duration


#time = np.linspace(0, sampling_duration, number_of_samples, endpoint=False)
# print("Subject : " + str(sub))

baseline_df = pd.read_csv('/content/drive/MyDrive/Datasets/Amigos/Mat_toCSV/User32/short_videos/' + file)
#baseline_df

column_names=['mean_gsr','var_gsr','skew_gsr','kurtosis_gsr','std_gsr','mean_scl','var_scl','skew_scl','kurtosis_scl','std_scl','slope_scl','mean_scr','var_scr','skew_scr','kurtosis_scr','std_scr','max_scr']
baseline_df=baseline_df.drop(['AF3','F7','F3','FC5','T7','P7','O1','O2','P8','T8','FC6','F4','F8','AF4','ECG_Right','ECG_Left'],axis=1)

#for base in baseline_file[1:]:
#    data=pd.read_csv(base)
#    baseline_df=baseline_df.append(data,ignore_index=True)

#baseline_df

df2 = baseline_df.tail(6400)
#df2

#labelling_criteria=1

result_baseline_df=windowing(df2,512)
#result_baseline_df['stress_level']=1;
result_baseline_df.to_csv("/content/drive/MyDrive/Datasets/Amigos/Mat_toCSV/User32/short_videos/Gsr_features/GSR32_vid_15.csv", index=False, encoding='utf-8-sig')
result_baseline_df.shape

# PATH = "/content/drive/MyDrive/Datasets/Amigos/GSR_Features"
# EXT = "*.csv"
# all_csv_files = [file
#                  for path, subdir, files in os.walk(PATH)
#                  for file in glob(os.path.join(path, EXT))]
# # len(all_csv_files)
# column_names=['mean_gsr','var_gsr','skew_gsr','kurtosis_gsr','std_gsr','mean_scl','var_scl','skew_scl','kurtosis_scl','std_scl','slope_scl','mean_scr','var_scr','skew_scr','kurtosis_scr','std_scr','max_scr','scr_peaks']

# combineddf= pd.DataFrame(columns=column_names)

# for featuredata in all_csv_files:
#       tempdata=pd.read_csv(featuredata)
#       combineddf=combineddf.append(tempdata,ignore_index=True)

# combineddf.to_csv("/content/drive/MyDrive/Datasets/Amigos/GSR_Features/GSR_features_combined_all_users.csv")

# combineddf
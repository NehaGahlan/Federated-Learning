# -*- coding: utf-8 -*-
"""FedAvg-code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AYDBNBT6L-Bawj6bnKwUdPwh6Kh2r2gJ

**FedAvg Algorithm**

1. Initialize the model parameters $w_0$.
2. Repeat for $t = 1,2,...,T$:
  - Select a random subset $S_t$ of clients from the available clients.
  - For each client $k \in S_t$, do the following:
      - Send the current model parameters $w_{t-1}$ to client $k$.
      - Client $k$ performs $E$ epochs of local stochastic gradient descent (SGD) on its local dataset with learning rate $\eta$ and initial model parameters $w_{t-1}$, and obtains the updated model parameters $w_{t,k}$.
      - Send the updated model parameters $w_{t,k}$ back to the server.
3. At the server, calculate the new global model parameters as the weighted average of the client models:
$w_t \leftarrow \sum_{k \in S_t} \frac{n_k}{n} w_{t,k}$,
where $n_k$ is the number of samples in client $k$'s dataset, $n = \sum_{k \in S_t} n_k$ is the total number of samples in the selected clients' datasets.
4. Return the final model parameters $w_T$.

**FedAvg Psuedocode**

1. initialize global model parameters w_0
2. for t = 1, 2, ..., T do:
    - select a random subset S_t of clients
      - for each client k in S_t do:
        - send w_{t-1} to client k
        - client k performs E epochs of local SGD on its local dataset with learning rate eta and initial model parameters w_{t-1}, and obtains updated model parameters w_{t,k}
        - send w_{t,k} back to server
    - calculate the new global model parameters as the weighted average of the client models:
        w_t <- sum(n_k/n) * w_{t,k}, for k in S_t
        where n_k is the number of samples in client k's dataset, n = sum(n_k) is the total number of samples in the selected clients' datasets
3. return final model parameters w_T
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_federated as tff
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from sklearn.datasets import make_classification

def create_keras_model():
    input_shape = 168
    return tf.keras.models.Sequential([
                                       Dense(128, input_dim=input_shape, activation='relu'),
                                       Dropout(0.2),
                                       Dense(64, input_dim=input_shape, activation='relu'),
                                       Dropout(0.2),
                                       ])

def model_fn():
  keras_model = create_keras_model()
  return tff.learning.from_keras_model(
      keras_model,
      input_spec=example_data.element_spec,
      loss=tf.keras.losses.BinaryCrossentropy(),
      metrics=[tf.keras.metrics.BinaryAccuracy()])

iterative_process = tff.learning.build_federated_averaging_process(
    model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.02),
    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05))

print(iterative_process.initialize.type_signature.formatted_representation())

state = iterative_process.initialize()
state, metrics = iterative_process.next(state, federated_train_data)
print('round  1, metrics={}'.format(metrics))

NUM_ROUNDS = 50
for round_num in range(1, NUM_ROUNDS):
  state, metrics = iterative_process.next(state, federated_train_data)
  print('round {:2d}, metrics={}'.format(round_num, metrics))

"""Evaluation"""

def create_test_data(client_ids):
    x = tf.concat([data_dict[id] for id in client_ids], 0)
    y = tf.concat([target_dict[id] for id in client_ids], 0)
    return x, y
    
test_data_x, test_data_y = create_test_data(test_clients)
test_data_x.shape, test_data_y.shape

test_data = make_federated_data(test_clients)

NUM_ROUNDS = 50

# The state of the FL server, containing the model and optimization state.
state = iterative_process.initialize()

# Global model
keras_model = create_keras_model()

# Load our pre-trained Keras model weights into the global model state.
state = tff.learning.state_with_new_model_weights(
    state,
    trainable_weights=[v.numpy() for v in keras_model.trainable_weights],
    non_trainable_weights=[
        v.numpy() for v in keras_model.non_trainable_weights
    ])


def keras_evaluate(state, round_num):
  # Take our global model weights and push them back into a Keras model to
  # use its standard `.evaluate()` method.
  #keras_model = load_model(batch_size=256)
  keras_model = create_keras_model()
  keras_model.compile(
      loss=tf.keras.losses.BinaryCrossentropy(),
      metrics=[tf.keras.metrics.BinaryAccuracy()])
  state.model.assign_weights_to(keras_model)
  loss, accuracy = keras_model.evaluate(x=test_data_x, y=test_data_y, verbose=0)
  print('\tEval: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))


for round_num in range(NUM_ROUNDS):
  print('Round {r}'.format(r=round_num))
  keras_evaluate(state, round_num)
  state, metrics = iterative_process.next(state, federated_train_data)
  train_metrics = metrics['train']
  print('\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(
      l=train_metrics['loss'], a=train_metrics['binary_accuracy']))

print('Final evaluation')
keras_evaluate(state, NUM_ROUNDS + 1)
# -*- coding: utf-8 -*-
"""Propsed-AttentionFL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/194xASVB9ZZ4cp8x9IHlQXTxOaWMTRCiD
"""

import time 
start_time = time.time()
!pip install --quiet --upgrade tensorflow_federated

!pip install --quiet --upgrade nest-asyncio

import nest_asyncio
nest_asyncio.apply()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

!pip install tensorflow-federated==0.20.0

!pip install tensorflow==2.9.2

import numpy as np
import pandas as pd
from collections import OrderedDict
import random

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import tensorflow as tf
import tensorflow_federated as tff
from keras import layers
from keras import regularizers
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Dropout
from keras.models import load_model
from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, concatenate
from keras.models import Model
from keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow import keras
from tensorflow.keras import layers

from google.colab import drive
drive.mount('/content/drive')

"""# Globals"""

SEED = 42

AMIGOS_PATH = '/content/drive/MyDrive/Datasets/AMIGOS/Global_Data/'

data = pd.read_csv((AMIGOS_PATH + 'X.csv', index_col = 0)

target_df = pd.read_csv(AMIGOS_PATH + 'y.csv', index_col = 0)
target_df = target_df.drop(['Arousal','Valence','Video'], axis = 1)

target_df[target_df['Dominance'] <= 4.5] = 0
target_df[target_df['Dominance'] > 4.5] = 1

target_df

data = np.array(data)
target = np.array(target_df)

data.shape, target.shape

# Make dictionary with each subject

data_dict = dict()
target_dict = dict()

for idx in range(40):
    data_dict[idx] = data[16*idx : 16*(idx+1)]
    target_dict[idx] = target[16*idx : 16*(idx+1)]

len(data_dict), len(target_dict)

data_dict[0][4] == data[4], target_dict[0][4] == target[4]

subjects = list(range(40))
random.Random(SEED).shuffle(subjects)
print(subjects)
# data points > # clients
# Using client size = 6, # clients = 3

train_clients = subjects[:36]
test_clients = subjects[36:]
train_clients, test_clients

NO_OF_CLIENTS = 3

client_ids = list(range(3))
client = [0, 0, 0]
client[0] = train_clients[:12]
client[1] = train_clients[12:24]
client[2] = train_clients[24:36]

for i in range(3):
    print(f'Client #{i}, Subjects: {client[i]}')

clientwise_data = dict()
clientwise_target = dict()

for i in range(NO_OF_CLIENTS):
    clientwise_data[i] = tf.concat([data_dict[id] for id in client[i]], 0)
    clientwise_target[i] = tf.concat([target_dict[id] for id in client[i]], 0)

def preprocess(client_id):
    def argument_free_preprocess():
        yield OrderedDict(
            # x = tf.expand_dims(data_dict[client_id], axis=0),
            # y = tf.expand_dims(target_dict[client_id], axis=0)
            x = clientwise_data[client_id],
            y = clientwise_target[client_id]
        )
    return argument_free_preprocess

def make_federated_data(client_ids):
  return [
      preprocess(x)
      for x in client_ids
  ]

federated_train_data = make_federated_data(client_ids)

def example_preprocess(client_id):
    return OrderedDict(
        x = tf.expand_dims(clientwise_data[client_id], axis=0),
        y = tf.expand_dims(clientwise_target[client_id], axis=0)
        # x = data_dict[client_id],
        # y = target_dict[client_id]
    )
example_data = example_preprocess(0)
example_data = tf.data.Dataset.from_tensor_slices(example_data)
example_data.element_spec

# Define the Attention layer
class Attention(layers.Layer):
    def __init__(self, hidden_dim):
        super(Attention, self).__init__()
        self.hidden_dim = hidden_dim
        self.attention_weights = layers.Dense(hidden_dim, activation='tanh')
        self.attention_values = layers.Dense(1)
    
    def call(self, inputs):
        attention_score = self.attention_weights(inputs)
        attention_weights = tf.nn.softmax(self.attention_values(attention_score), axis=1)
        context_vector = attention_weights * inputs
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector

# Define the model architecture
def create_keras_model_with_attention():
  model = keras.Sequential()
  model.add(layers.Dense(128, activation='relu', input_shape=(168,)))
  model.add(layers.Dropout(0.2))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dropout(0.2))
  
  # Connect the attention layer between the hidden layer and the output layer
  model.add(Attention(2))
  return model.add(layers.Dense(1, activation='sigmoid'))

def model_fn():
  keras_model = create_keras_model_with_attention()
  return tff.learning.from_keras_model(
      keras_model,
      input_spec=example_data.element_spec,
      loss=tf.keras.losses.BinaryCrossentropy(),
      metrics=[tf.keras.metrics.BinaryAccuracy()])

iterative_process = tff.learning.build_federated_averaging_process(
    model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.02),
    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05))

print(iterative_process.initialize.type_signature.formatted_representation())

!pip install nest_asyncio
import nest_asyncio
nest_asyncio.apply()

state = iterative_process.initialize()

state, metrics = iterative_process.next(state, federated_train_data)
print('round  1, metrics={}'.format(metrics))

NUM_ROUNDS = 50
# import time 
# start_time = time.time()
for round_num in range(1, NUM_ROUNDS):
  state, metrics = iterative_process.next(state, federated_train_data)
  print('round {:2d}, metrics={}'.format(round_num, metrics))
#   end_time = time.time()

# print("Training time:", end_time - start_time, "seconds")

"""# Tensorboard"""

logdir = "/tmp/logs/scalars/training/"
summary_writer = tf.summary.create_file_writer(logdir)
state = iterative_process.initialize()

with summary_writer.as_default():
  for round_num in range(1, NUM_ROUNDS):
    state, metrics = iterative_process.next(state, federated_train_data)
    for name, value in metrics['train'].items():
      tf.summary.scalar(name, value, step=round_num)

!ls

# Commented out IPython magic to ensure Python compatibility.
!ls {logdir}
# %tensorboard --logdir {logdir} --port=0

"""# Evaluation"""

# def example_preprocess(client_id):
#     return OrderedDict(
#         x = tf.expand_dims(data_dict[client_id], axis=0),
#         y = tf.expand_dims(target_dict[client_id], axis=0)
#         # x = data_dict[client_id],
#         # y = target_dict[client_id]
#     )
# example_data = example_preprocess(0)
# example_data = tf.data.Dataset.from_tensor_slices(example_data)

def create_test_data(client_ids):
    x = tf.concat([data_dict[id] for id in client_ids], 0)
    y = tf.concat([target_dict[id] for id in client_ids], 0)
    return x, y
    
test_data_x, test_data_y = create_test_data(test_clients)
test_data_x.shape, test_data_y.shape

test_data = make_federated_data(test_clients)

import time
NUM_ROUNDS = 100

start_time = time.time()

# The state of the FL server, containing the model and optimization state.
state = iterative_process.initialize()

# Global model
keras_model = create_keras_model_with_attention()

# Load our pre-trained Keras model weights into the global model state.
state = tff.learning.state_with_new_model_weights(
    state,
    trainable_weights=[v.numpy() for v in keras_model.trainable_weights],
    non_trainable_weights=[
        v.numpy() for v in keras_model.non_trainable_weights
    ])

def keras_evaluate(state, round_num):
  # Take our global model weights and push them back into a Keras model to
  # use its standard `.evaluate()` method.
  #keras_model = load_model(batch_size=256)
  keras_model = create_keras_model_with_attention()
  keras_model.compile(
      loss=tf.keras.losses.BinaryCrossentropy(),
      metrics=[tf.keras.metrics.BinaryAccuracy()])
  state.model.assign_weights_to(keras_model)
  loss, accuracy = keras_model.evaluate(x=test_data_x, y=test_data_y, verbose=0)
  print('\tEval: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))


for round_num in range(NUM_ROUNDS):
  print('Round {r}'.format(r=round_num))
  keras_evaluate(state, round_num)
  state, metrics = iterative_process.next(state, federated_train_data)
  train_metrics = metrics['train']
  print('\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(
      l=train_metrics['loss'], a=train_metrics['binary_accuracy']))
   print('\tTrain: {}'.format(train_metrics))

print('Final evaluation')
keras_evaluate(state, NUM_ROUNDS + 1)
 end_time = time.time()

print("Training time:", end_time - start_time, "seconds")

np.sum(test_data_y) / len(test_data_y)
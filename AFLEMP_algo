Algorithm for Attention-based Federated Learning for Emotion recognition using Multi-Modal Physiological data (AFLEMP)

Input: Training data ğ· = {(ğ‘¥1, ğ‘¦1), â€¦, (ğ‘¥ğ‘›, ğ‘¦ğ‘›)}, number of clients ğ¾ (K=3, 5, 7, 10, ..20), number of communication rounds ğ‘‡ (T= 50, 100 and
200), learning rate ğœ‚, batch size ğµ, weighted aggregation method, attention mechanism.
Output: Trained model ğ‘¤âˆ—
1: Initialize global model parameters ğ‘¤0
2: for each communication round ğ‘¡ = 1, â€¦, ğ‘‡ do
3: for each client ğ‘˜ = 1,â€¦ , ğ¾ in parallel do
4: Client ğ‘˜ selects a random subset ğµğ‘˜ âŠ‚ ğ· of size ğµ.
5: Client ğ‘˜ computes the local gradient: ğ‘”ğ‘˜ â† âˆ‡ğ‘“ğ‘˜(ğ‘¤ğ‘¡âˆ’1;ğµğ‘˜), where ğ‘“ğ‘˜ is the local loss function.
6: Client ğ‘˜ computes the attention weights: ğ‘ğ‘˜ â† softmax(ğ–ğ‘ğ±ğ‘˜), where ğ–ğ‘ is the attention matrix and ğ±ğ‘˜
is the local feature vector.
7: Client ğ‘˜ computes the weighted gradient: Ìƒğ‘”ğ‘˜ â† ğ‘ğ‘˜ğ‘”ğ‘˜
8: Client ğ‘˜ sends the weighted gradient Ìƒğ‘”ğ‘˜ to the server.
9: end for
10: Server aggregates the weighted gradients: Ìƒğ‘” â† âˆ‘ğ¾ ğ‘˜=1 Ìƒğ‘”ğ‘˜ using the weighted aggregation method (Eq. (13)).
11: Server updates the global model: ğ‘¤ğ‘¡ â† ğ‘¤ğ‘¡âˆ’1 âˆ’ ğœ‚ Ìƒğ‘”.
12: end for
13: return ğ‘¤âˆ— â† ğ‘¤ğ‘‡
